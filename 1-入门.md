# 入门 lenet



![image-20220523222302511](images/1-%E5%85%A5%E9%97%A8.assets/image-20220523222302511.png)

LeNet 系列包含了以下三个模型：

- LeNet-1：5 层模型，一个简单的 CNN。
- LeNet-4：6 层模型，是 LeNet-1 的改进版本。
- LeNet-5：7 层模型，最著名的版本。



CNN 的设计是为了模拟人眼的感知方式。传统的 CNN 一般包含以下三种层：

- 卷积层（convolution layers）
- 降采样层（subsampling layers），或称池化层（pooling layers）
- 全连接层（fully connected layers）



LeNet是于1989年提出的一种网络结构，目前在各大深度学习框架中使用的是简化版的LeNet-5，与原始的结构区别在于把激活函数换成了ReLU；

LeNet奠定了卷积层+池化层+全连接层的结构，初用于手写数字的识别问题，输入为单通道的灰度图。

![101fe89c6da37e96fca94d4c22630555](images/1-%E5%85%A5%E9%97%A8.assets/101fe89c6da37e96fca94d4c22630555.png)

# 模型解析

## LeNet网络结构详解与模型的搭建

- LeNet分为卷积层块和全连接层块两个部分。
- 卷积层块⾥的基本单位是卷积层后接最⼤池化层
- 卷积层⽤来识别图像⾥的空间模式，如线条和物体局部，
- 最⼤池化层则⽤来降低卷积层对位置的敏感性。
- LeNet组成 卷积层、 下采样层、 卷积层、 下采样层、 三个全连接层

![20170906212851976](images/1-%E5%85%A5%E9%97%A8.assets/20170906212851976.jpeg)

**LeNet-5的输入和输出**如下：

**输入**：size为32×32像素大小的手写体字符图片（这些手写体包含从0-9的数字，即一共有10个类别的图片）
**输出**：分类结果（即0-9之间的一个数）

**LeNet-5的结构**如上图所示，网络**一共有7层（不包含输入层）**：

图中的**Convolutions**代表**卷积**、**Subsampling**代表**降采样**（也就是我们上面介绍的**池化**操作）、**Full connection**代表**全连接层**

![暂时不要](imagess/1-%E5%85%A5%E9%97%A8.assets/dl_3_1.png)



## 网络详解

![image-8](images/1-%E5%85%A5%E9%97%A8.assets/image-8.png)

　在 MNIST 数据集上，Yann LeCun 的 LeNet-5 具有 0.95% 的低误判率。其各层参数如下：

- C1：卷积层，num_kernels=6, kernel_size=5×5, padding=0, stride=1
- S2：均值池化层，kernel_size=2×2, padding=0, stride=2
- C3：卷积层，num_kernels=16, kernel_size=5×5, padding=0, stride=1
- S4：均值池化层，kernel_size=2×2, padding=0, stride=2
- F5：全连接层，out_features=140
- F6：全连接层，out_features=84
- F7：全连接层，out_features=10

**0、输入层 INPUT：**

输入二维图像，尺寸统一归一化为32×32
**注意：本层不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。**

**1、C1层-卷积层：**

该层的作用为提取特征，

输入图片：32 * 32

卷积核大小：5 * 5

卷积核数量：6

输出feature map大小：28 * 28  <\==\====（32-5+1）=28

神经元数量[<sup>补充-1</sup>](#refer-anchor-1)：28 * 28 * 6  

可训练参数[<sup>补充-1</sup>](#refer-anchor-1)：（5 * 5+1) * 6（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器）

连接数：（5 * 5+1）\* 6 \* 28 \* 28=122304

**详细说明：**对输入图像进行第一次卷积运算（使用 6 个大小为 5 \* 5 的卷积核），得到6个C1特征图（6个大小为28 \* 28的 feature maps, 32-5+1=28）。我们再来看看需要多少个参数，卷积核的大小为5 \* 5，总共就有6 \*（5 \* 5+1）=156个参数，其中+1是表示一个核有一个bias。对于卷积层C1，C1内的每个像素都与输入图像中的5 \* 5个像素和1个bias有连接，所以总共有156 \* 28 \* 28=122304个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。

（最后顺便说一下为什么要进行卷积？卷积运算一个重要的特点就是，通过卷积运算，可以使原信号特征增强，并且降低噪音，同时不同的卷积层可以提取到图像中的不同特征，这层卷积我们就是用了6个卷积核）

**2、S2层-池化层（下采样层）**

该层的作用为缩小矩阵的尺寸，减小计算量。

池化不影响通道数，所以输出的channel还是6，每张feature map都是14×14，神经元数量为14×14×6

输入：28*28

采样区域：2*2

采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid

采样种类：6  # 下采样不影响通道数量，采样种类就类似于卷积里面的卷积核的个数

输出featureMap大小：14*14（28/2）

神经元数量：14\*14\*6

连接数：（2\*2+1）\*6\*14\*14

S2中每个特征图的大小是C1中特征图大小的1/4。

**详细说明：**第一次卷积之后紧接着就是池化运算，使用 2\*2核 进行池化，于是得到了S2，6个14\*14的 特征图（28/2=14）。S2这个pooling层是对C1中的2*2区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。同时有5x14x14x6=5880个连接。

（说明下为什么要进行下采样？利用图像局部相关性的原理，对图像进行子抽样，可以减少数据处理量同时保留有用信息，从而降低了网络训练的参数和模型的过拟合程度）

一般神经网络常采用最大池化或平均池化。子采样具体过程如下：每邻域四个像素求和变为一个像素，然后通过标量${w_{x+1}}$加权，再增加偏置$b_{x+1}$，然后通过一个sigmoid激活函数，产生一个大概缩小四倍的特征映射图$S_{x+1}$。在整个网络中，S-层可看作是模糊滤波器，起到二次特征提取的作用。隐层与隐层之间空间分辨率递减，而每层所含的平面数递增，这样可用于检测更多的特征信息。

**3、C3层-卷积层**

该层的作用还是提取特征,

输入：S2中所有6个或者几个特征map组合

卷积核大小：5*5

卷积核种类：16

输出featureMap大小：10*10 (14-5+1)=10

神经元数量：10×10×16

C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合

存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。

则：可训练参数：6\*(3\*5\*5+1)+6\*(4\*5\*5+1)+3\*(4\*5\*5+1)+1\*(6\*5\*5+1)=1516

连接数：10\*10\*1516=151600

**详细说明：**第一次池化之后是第二次卷积，第二次卷积的输出是C3，16个10x10的特征图，卷积核大小是 5\*5. 我们知道S2 有6个 14*14 的特征图，怎么从6 个特征图得到 16个特征图了？ 这里是通过对S2 的特征图特殊组合计算得到的16个特征图。具体如下：

![dl_3_5](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_5.png)

​															每一列是一个输出通道，有「X」的行表示与之相连的 S2 通道

C3的前6个feature map（对应上图第一个红框的6列）与S2层相连的3个feature map相连接（上图第一个红框），后面6个feature map与S2层相连的4个feature map相连接（上图第二个红框），后面3个feature map与S2层部分不相连的4个feature map相连接，最后一个与S2层的所有feature map相连。卷积核大小依然为5\*5，所以总共有6\*(3\*5\*5+1)+6\*(4*5\*5+1)+3\*(4\*5\*5+1)+1\*(6\*5\*5+1)=1516个参数。而图像大小为10\*10，所以共有151600个连接。

<img src="images/1-%E5%85%A5%E9%97%A8.assets/dl_3_9.png" alt="dl_3_9" style="zoom:150%;" />

C3与S2中前3个图相连的卷积结构如下图所示

![dl_3_6](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_6.png)

上图对应的参数为 3\*5\*5+1，一共进行6次卷积得到6个特征图，所以有6\*（3\*5\*5+1）参数。 为什么采用上述这样的组合了？论文中说有两个原因：1）减少参数，2）这种不对称的组合连接的方式有利于提取多种组合特征。

**4、S4层-池化层（下采样层）**

该层的作用为缩小矩阵的尺寸，减小计算量,

输入：10*10

采样区域：2*2

采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid

采样种类：16

输出featureMap大小：5*5（10/2）

神经元数量：5\*5\*16=400

连接数：16\*（2\*2+1）\*5\*5=2000

S4中每个特征图的大小是C3中特征图大小的1/4

**详细说明：**S4是pooling层，窗口大小仍然是2*2，共计16个feature map，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。有5x5x5x16=2000个连接。连接的方式与S2层类似。

**5、C5层-卷积层**

输入：S4层的全部16个单元特征map（与s4全相连）

卷积核大小：5*5

卷积核种类：120

输出featureMap大小：1*1（5-5+1）

可训练参数/连接：120\*（16\*5\*5+1）=48120

神经元数量：1×1×120=120

**详细说明：**

**（其实也可以理解成全连接层，因为这里5×5的输入，用5×5的卷积核做卷积，最后输出1×1）**

这样刚好变成了全连接，但是我们不把它写成F5，因为这只是巧合。

C5层是一个卷积层。由于S4层的16个图的大小为5x5，与卷积核的大小相同，所以卷积后形成的图的大小为1x1。这里形成120个卷积结果。每个都与上一层的16个图相连。所以共有(5x5x16+1)x120 = 48120个参数，同样有48120个连接。C5层的网络结构如下：

![dl_3_7](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_7.png)



**6、F6层-全连接层**

输入：c5 120维向量

计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。

可训练参数:84*(120+1)=10164

F6层有84个输出节点

**详细说明：**6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，-1表示白色，1表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。ASCII编码图如下：

![dl_3_8](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_8-16534444133228.png)

F6层的连接方式如下：

![dl_3_9](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_9-165344445568910.png)

**7、全连接层Output**

Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：

![dl_3_10](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_10.png)

上式w_ij 的值由i的比特图编码确定，i从0到9，j取值从0到7*12-1。RBF输出的值越接近于0，则越接近于i，即越接近于i的ASCII编码图，表示当前网络输入的识别结果是字符i。该层有84x10=840个参数和连接。

整个过程的feature map大致如下：

![dl_3_11](images/1-%E5%85%A5%E9%97%A8.assets/dl_3_11.png)



上图是LeNet-5识别数字3的过程。

参考链接：https://cuijiahua.com/blog/2018/01/dl_3.html

https://www.ruanx.net/lenet/

# 代码

1. model.py ——定义LeNet网络模型
2. train.py ——加载数据集并训练，训练集计算loss，测试集计算accuracy，保存训练好的网络参数
3. predict.py——得到训练好的网络参数后，用自己找的图像进行分类测试

## model.py中代码解读

见code

```python
import torch.nn as nn
import torch.nn.functional as F

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__() 
        #super函数解决在多种继承中更好的调用父类super在多继承中经常使用 
        self.conv1 = nn.Conv2d(3, 16, 5)#可以在pytorch官网Docs中查找Conv2d可以找到函数详细的定义#
        #可以显示计算公式 3代表输入特征层的深度，用了16个卷积核、卷积核大小是5*5 
        self.pool1 = nn.MaxPool2d(2, 2)#点击查看MaxPool2d的函数定义，发现没有初始化函数、
        #跳转看其父类MaxPoolNd才能看到初始化参数 （self,kernel_size,，stride=None，padding=0,dilation=1,return_indices=False,ceil_mode=False)
        #如果没有传入stride参数、那stride=kernal_size卷积核大小   
        #这里的2代表卷积核大小，2代表步长 
        self.conv2 = nn.Conv2d(16, 32, 5)#16深度 32卷积核个数 5*%卷积核大小
        self.pool2 = nn.MaxPool2d(2, 2) #这里的2代表卷积核大小，2代表步长 
        self.fc1 = nn.Linear(32*5*5, 120) #全连接层的输入是一个一维的向量，这里需要展平所以是32*5*5 深度*高度*宽度 节点个数为120
        self.fc2 = nn.Linear(120, 84) #上一层的输出120 第二层设计输出84
        self.fc3 = nn.Linear(84, 10)  #上一层的输出84 第二层设计输出需要根据我们的训练集合进行修改，用来实验的具有10个分类任务，所以设为10
      
     def forward(self, x):
        x = F.relu(self.conv1(x))    # input(3, 32, 32) output(16, 28, 28)
        # （深度，高度、宽度） N = 28 = （ 32 - 5 + 0 ）/1 + 1     16 = 卷积核的个数 
        x = self.pool1(x)            # output(16, 14, 14) 池化层只会影响特征矩阵的高和宽不会影响深度，高度和宽度缩小为原来的一半
        x = F.relu(self.conv2(x))    # output(32, 10, 10)  
        # N = 10 = （ 14 - 5 + 0 ）/1 + 1 32卷积核个数
        x = self.pool2(x)            # output(32, 5, 5) #高度和宽度缩小为原来的一半
        x = x.view(-1, 32*5*5)       # output(32*5*5) #view函数可以用于自动展平数据 -1代表第一个维度batch 32*5*5是展平后的数据个数
        x = F.relu(self.fc1(x))      # output(120)
        x = F.relu(self.fc2(x))      # output(84)
        x = self.fc3(x)              # output(10)
        return x #为什么这个分类问题没有使用softmax函数，但是在训练网络过程中计算卷积交叉熵时已经内部实现了一个搞笑的softmax方法

  调试信息
import torch
input1 = torch.rand([32, 3, 32, 32]) # [batchsize, channel, height, width]
model = LeNet()
print(model)
output = model(input1)
# 可以  x = F.relu(self.conv1(x)) 设置断点进行调试


```



## train.py代码解读

见code

使用到的数据集

![20200824190613186](images/1-%E5%85%A5%E9%97%A8.assets/20200824190613186.png)

## predict.py代码 解读

见code





----

#### 截止日期																		

​																																																			            2022-05-25











